{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "W3_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CIS-522/course-content/blob/main/tutorials/W3_MLPs/W3_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2kPrtVyXtaP"
      },
      "source": [
        "# CIS-522 Week 3 Part 1\r\n",
        "# Multi-Layer Perceptrons (MLPs)\r\n",
        "\r\n",
        "__Instructor__: Konrad Kording\r\n",
        "\r\n",
        "__Content creators:__ Arash Ash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2DuJddRXxYY"
      },
      "source": [
        "---\r\n",
        "# Tutorial Objectives\r\n",
        "In this tutorial, we delve deeper by using one of the most famous deep learning models of all!\r\n",
        "\r\n",
        "MLPs are arguably one of the most tractable models that we can use to study deep learning fundamentals. Here we will learn why MLPs are: \r\n",
        "\r\n",
        "* similar to biological networks\r\n",
        "* good at function approximation\r\n",
        "* can evolve linearly in weights \r\n",
        "* the case of deep vs. wide\r\n",
        "* dependant on transfer functions\r\n",
        "* sensitive to initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "e6GnDC12UtUD"
      },
      "source": [
        "#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n",
        "my_pennkey = 'value' #@param {type:\"string\"}\n",
        "my_pod = 'Select' #@param ['Select', 'euclidean-wombat', 'sublime-newt', 'buoyant-unicorn', 'lackadaisical-manatee','indelible-stingray','superfluous-lyrebird','discreet-reindeer','quizzical-goldfish','astute-jellyfish','ubiquitous-cheetah','nonchalant-crocodile','fashionable-lemur','spiffy-eagle','electric-emu','quotidian-lion']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np4EZarUtKoA"
      },
      "source": [
        "# Recap the experience from last week\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wEdRnCm2_DT0"
      },
      "source": [
        "#@title Video: Discussing Week 2 - Linear DL\n",
        "import time\n",
        "try: t0;\n",
        "except NameError: t0=time.time()\n",
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "video = YouTubeVideo(id=\"xlYttP5C_LY\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NtskSZXZqu3W"
      },
      "source": [
        "# @title Slides\r\n",
        "from IPython.display import HTML\r\n",
        "HTML('<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSPvHqDTmMq4GyQy6lieNEFxq4qz1SmqC2RNoeei3_niECH53zneh8jJVYOnBIdk0Uaz7y2b9DK8V1t/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"480\" height=\"299\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E29pz3hNeQTO"
      },
      "source": [
        "\r\n",
        "Meet with your pod for 10 minutes to discuss what you learned, what was clear, and what you hope to learn more about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "c8nOPFKTuJhA"
      },
      "source": [
        "#@markdown Tell us your thoughts about what you have learned.\n",
        "my_w2_upshot = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLzK3-6nvj9U"
      },
      "source": [
        "# Question of the week\r\n",
        "What functional forms are good or bad for representing complex functions?\r\n",
        "\r\n",
        "[answers include differentiable, hierarchical, smooth, dynamical, nonlinear]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgOQxVS1X2dB"
      },
      "source": [
        "---\r\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXd8Now_XwR7"
      },
      "source": [
        "# imports\r\n",
        "import random\r\n",
        "import pathlib\r\n",
        "\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torchvision.datasets import ImageFolder\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from IPython.display import HTML, display\r\n",
        "\r\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "dev, torch.get_num_threads()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZUyEQpxTSzxe"
      },
      "source": [
        "# @title Seeding for reproducibility\r\n",
        "seed = 522\r\n",
        "random.seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "torch.manual_seed(seed)\r\n",
        "torch.cuda.manual_seed(seed)\r\n",
        "\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "torch.backends.cudnn.benchmark = False\r\n",
        "torch.set_deterministic(True)\r\n",
        "def seed_worker(worker_id):\r\n",
        "    worker_seed = seed % (worker_id+1)\r\n",
        "    np.random.seed(worker_seed)\r\n",
        "    random.seed(worker_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hj8C2Cf0G0i_"
      },
      "source": [
        "# @title Dataset download\r\n",
        "%%capture\r\n",
        "!rm -r AnimalFaces32x32/\r\n",
        "!git clone https://github.com/arashash/AnimalFaces32x32\r\n",
        "!rm -r afhq/\r\n",
        "!unzip ./AnimalFaces32x32/afhq_32x32.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oRrTcZOlXA8C"
      },
      "source": [
        "# @title Figure Settings\r\n",
        "import ipywidgets as widgets\r\n",
        "%matplotlib inline \r\n",
        "fig_w, fig_h = (8, 6)\r\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "my_layout = widgets.Layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "40Qc7MFcX7Oc"
      },
      "source": [
        "# @title Helper functions\r\n",
        "def imshow(img):\r\n",
        "    img = img / 2 + 0.5     # unnormalize\r\n",
        "    npimg = img.numpy()\r\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
        "    plt.axis(False)\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "def progress(epoch, loss, epochs=100):\r\n",
        "    return HTML(\"\"\"\r\n",
        "        <label for=\"file\">Training loss: {loss}</label>\r\n",
        "        <progress\r\n",
        "            value='{epoch}'\r\n",
        "            max='{epochs}',\r\n",
        "            style='width: 100%'\r\n",
        "        >\r\n",
        "            {epoch}\r\n",
        "        </progress>\r\n",
        "    \"\"\".format(loss=loss, epoch=epoch, epochs=epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQGBDFB0yeqV"
      },
      "source": [
        "---\r\n",
        "# Section 1: Neuron Physiology"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FNdeptL-zUnU"
      },
      "source": [
        "#@title Video: Overview and Integrate-and-Fire Neurons\n",
        "try: t1;\n",
        "except NameError: t1=time.time()\n",
        "\n",
        "video = YouTubeVideo(id=\"exTzHGfEAvU\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93MxeKdfGN7"
      },
      "source": [
        "## Section 1.1: Leaky Integrate and Fire (LIF) Neuron\r\n",
        "The basic idea of LIF neuron was proposed in 1907 by Louis Ã‰douard Lapicque, long before we understood the electrophysiology of a neuron (see a translation of [Lapicque's paper](https://pubmed.ncbi.nlm.nih.gov/17968583/) ). More details of the model can be found in the book [**Theoretical neuroscience**](http://www.gatsby.ucl.ac.uk/~dayan/book/) by Peter Dayan and Laurence F. Abbott.\r\n",
        "\r\n",
        "The model dynamic is defined with the following formula,\r\n",
        "\r\n",
        "$$\r\n",
        "\\frac{d V}{d t}=\\left\\{\\begin{array}{cc}\r\n",
        "\\frac{1}{R_{m}C_{m}}\\left(-V+I R_{m}\\right) & t>t_{r e s t} \\\\\r\n",
        "0 & \\text { otherwise }\r\n",
        "\\end{array}\\right.\r\n",
        "$$\r\n",
        "\r\n",
        "And If $I$ is sufficiently strong such that $V$ reaches a certain threshold value $V_{\\rm th}$, it momentarily spikes and then $V$ is reset to $V_{\\rm reset}< V_{\\rm th}$, and voltage stays at $V_{\\rm reset}$ for $\\tau_{\\rm ref}$ ms, mimicking the refractoriness of the neuron during an action potential:\r\n",
        "\r\n",
        "\\begin{eqnarray}\r\n",
        "V(t)=V_{\\rm reset} \\text{  for } t\\in(t_{\\text{sp}}, t_{\\text{sp}} + \\tau_{\\text{ref}}]\r\n",
        "\\end{eqnarray}\r\n",
        "\r\n",
        "where $t_{\\rm sp}$ is the spike time when $V(t)$ just exceeded $V_{\\rm th}$.\r\n",
        "\r\n",
        "For in-depth content, follow the [NMA](https://www.neuromatchacademy.org/) Week 3 Day 1 material on Real Neurons and specifically this [Tutorial](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/tutorials/W3D1_RealNeurons/W3D1_Tutorial1.ipynb)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW9mD1wC11Kw"
      },
      "source": [
        "## Exercise 1: Simulating an LIF Neuron\r\n",
        "\r\n",
        "Now, it's your turn to implement this simple mathematical model of a neuron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTydKkuOeqVc"
      },
      "source": [
        "def run_LIF(I, # input current (mA)\r\n",
        "            T = 50, # total time to simulate (msec)\r\n",
        "            dt = 0.1, # simulation time step (msec)\r\n",
        "            t_rest = 0, # initial refractory time\r\n",
        "            Rm = 1, # resistance (kOhm)\r\n",
        "            Cm = 10, # capacitance (uF)\r\n",
        "            tau_ref = 4, # refractory period (msec)\r\n",
        "            Vth = 1, # spike threshold (V)\r\n",
        "            V_spike = 0.5 # spike delta (V)\r\n",
        "            ):\r\n",
        "\r\n",
        "  # time constant (msec)\r\n",
        "  tau_m = Rm*Cm  # [TO-DO]\r\n",
        "  time = torch.arange(0, T+dt, dt) # [TO-DO]\r\n",
        "  \r\n",
        "  # potential (V) trace over time\r\n",
        "  Vm = torch.zeros(len(time)) \r\n",
        "\r\n",
        "  # iterate over each time step\r\n",
        "  for i, t in enumerate(time):\r\n",
        "    if t > t_rest:\r\n",
        "      Vm[i] = Vm[i-1] + (-Vm[i-1] + I*Rm) / tau_m * dt # [TO-DO]\r\n",
        "    if Vm[i] >= Vth:\r\n",
        "      Vm[i] += V_spike # [TO-DO]\r\n",
        "      t_rest = t + tau_ref # [TO-DO]\r\n",
        "  return time, Vm\r\n",
        "\r\n",
        "sim_time, Vm = run_LIF(1.5)\r\n",
        "# plot membrane potential trace\r\n",
        "plt.plot(sim_time, Vm)\r\n",
        "plt.title('LIF Neuron Output')\r\n",
        "plt.ylabel('Membrane Potential (V)')\r\n",
        "plt.xlabel('Time (msec)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5hljR2LZXxS"
      },
      "source": [
        "# Section 1.2: Nonlinearity of LIF Neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vh7Wnzg3ZYaY"
      },
      "source": [
        "#@title Video: Are Integrate-and-Fire Neurons Linear?\n",
        "\n",
        "video = YouTubeVideo(id=\"6IzHZB7xf34\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0w47-O6Arl"
      },
      "source": [
        "## Interactive Demo: F-I Explorer for different $R_m$\r\n",
        "We know that neurons communicate by modulating the spike count. Therefore it makes sense to characterize their spike count as a function of input current. This is called the neuron's input-output transfer function (so simply F-I curve). Let's plot the neuron's F-I curve and see how it changes with respect to the membrane resistance? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MPKolg9xjiHW"
      },
      "source": [
        "# @title\r\n",
        "\r\n",
        "# @markdown Make sure you execute this cell to enable the widget!\r\n",
        "\r\n",
        "@widgets.interact(Rm=widgets.FloatSlider(1., min=0.5, max=10., step=0.1, layout=my_layout))\r\n",
        "\r\n",
        "def plot_IF_curve(Rm):\r\n",
        "  T = 100 # total time to simulate (msec)\r\n",
        "  dt = 1 # simulation time step (msec)\r\n",
        "  Vth = 1 # spike threshold (V)\r\n",
        "  Is = torch.linspace(0, 2, 10)\r\n",
        "  spike_counts = []\r\n",
        "  for I in Is:\r\n",
        "    _, Vm = run_LIF(I, T = T, Vth = Vth, Rm=Rm)\r\n",
        "    spike_counts += [torch.sum(Vm > Vth)]\r\n",
        "\r\n",
        "  plt.plot(Is, spike_counts)\r\n",
        "  plt.title('LIF Transfer Function (I/F Curve)')\r\n",
        "  plt.ylabel('Spike count')\r\n",
        "  plt.xlabel('I (mA)')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkvnWkvTz2O5"
      },
      "source": [
        "---\n",
        "# Section 2: The need for MLPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "tOgsH7ly5ZU1"
      },
      "source": [
        "#@title Video: The XOR Problem\n",
        "\n",
        "try: t2;\n",
        "except NameError: t2=time.time()\n",
        "\n",
        "video = YouTubeVideo(id=\"PERmPT1cOP0\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEy_D2A53SV"
      },
      "source": [
        "## Exercise 2: Solving XOR \r\n",
        "\r\n",
        "* Play with the widget and observe that you can not solve XOR\r\n",
        "* Now add one hidden layer with three units, play with the widget, and set weights by hand to solve XOR perfectly.\r\n",
        "\r\n",
        "For the second part, you should set the weights by clicking on the connections and either type the value or use the up and down keys to change it by one increment. You could also do the same for the biases by clicking on the tiny square to each neuron's bottom left.\r\n",
        "Even though there are infinitely many solutions, a neat solution when $f(x)$ is ReLU: \r\n",
        "\r\n",
        "$$y = f(x_1)+f(x_2)-f((x_1+x_2))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ybfxBilvrWH_"
      },
      "source": [
        "# @title XOR Exercise\r\n",
        "from IPython.display import HTML\r\n",
        "HTML('<iframe width=\"1020\" height=\"660\" src=\"https://playground.arashash.com/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.91390&showTestData=false&discretize=false&percTrainData=90&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMJZuSjmsEQV"
      },
      "source": [
        "---\n",
        "# Section 3: Universal Function Approximation Theorem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_uKiWhgZCSgY"
      },
      "source": [
        "#@title Video: Universal Approximation\n",
        "try: t3;\n",
        "except NameError: t3=time.time()\n",
        "\n",
        "video = YouTubeVideo(id=\"XXXYxolMVdw\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j2qcuN8x32j"
      },
      "source": [
        "## Exercise 3: Function Approximation with ReLU\r\n",
        "We learned that one hidden layer MLPs are enough to approximate any smooth function! Now let's manually fit a Sine function using ReLU activation. The idea is to set the weights iteratively so that the slope changes in the new sample's direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVSelFbmFIE1"
      },
      "source": [
        "N_test = 1000\r\n",
        "x_test = torch.linspace(0, 2*np.pi, N_test).view(-1, 1)\r\n",
        "y_test = torch.sin(x_test)\r\n",
        "plt.plot(x_test, y_test, label='ground truth')\r\n",
        "\r\n",
        "N_train = 10\r\n",
        "x_train = torch.linspace(0, 2*np.pi, N_train).view(-1, 1)\r\n",
        "y_train = torch.sin(x_train)\r\n",
        "\r\n",
        "W1 = torch.ones(1, N_train)  # [TO-DO]\r\n",
        "b1 = - x_train.view(N_train)  # [TO-DO]\r\n",
        "\r\n",
        "W2 = torch.zeros(N_train, 1)  # [TO-DO]\r\n",
        "prev_slope = 0\r\n",
        "for i in range(N_train-1):\r\n",
        "  delta_x = x_train[i+1] - x_train[i]  # [TO-DO]\r\n",
        "  slope = (y_train[i+1] - y_train[i]) / delta_x  # [TO-DO]\r\n",
        "  W2[i] = slope - prev_slope  # [TO-DO]\r\n",
        "  prev_slope = slope\r\n",
        "\r\n",
        "y_hat = torch.relu(b1 + x_test @ W1) @ W2  # [TO-DO]\r\n",
        "\r\n",
        "plt.plot(x_test, y_hat, label='estimated')\r\n",
        "plt.legend()\r\n",
        "plt.xlabel('x')\r\n",
        "plt.ylabel('y(x)')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eglZ3_zBEKHR"
      },
      "source": [
        "# Section ?: Completeness proof sketch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "f7_oWnsqERUk"
      },
      "source": [
        "#@title Video: Making Multi-Layer Perceptrons\n",
        "video = YouTubeVideo(id=\"bAhrg8Z8_r8\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b37WDpWIUNU"
      },
      "source": [
        "# MLPs in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "it4t-UVzJeop"
      },
      "source": [
        "#@title Video 5:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_TpUE0cHW1K"
      },
      "source": [
        "## Exercise 4: Implement a general-purpose MLP in Pytorch\r\n",
        "The objective is to design an MLP with these properties:\r\n",
        "* works with any input (1D, 2D, etc.)\r\n",
        "* construct any number of given hidden layers using ModuleList\r\n",
        "* use the same given activation function in all hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUsq7SdqHx8a"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self, actv, num_inputs, hidden_units, num_outputs):\r\n",
        "        super(Net, self).__init__()\r\n",
        "\r\n",
        "        exec('self.actv = nn.%s'%actv)   # [TO-DO]\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList()\r\n",
        "        for i in range(len(hidden_units)):\r\n",
        "          next_num_inputs = hidden_units[i] \r\n",
        "          self.layers += [nn.Linear(num_inputs, next_num_inputs)]   # [TO-DO]\r\n",
        "          num_inputs = next_num_inputs\r\n",
        "\r\n",
        "        self.out = nn.Linear(num_inputs, num_outputs)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # flattening\r\n",
        "        x = x.view(x.shape[0], -1)   # [TO-DO]\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "          x = self.actv(layer(x))  # [TO-DO]\r\n",
        "        x = self.out(x) # [TO-DO]\r\n",
        "        return x\r\n",
        "# test it\r\n",
        "Net(actv='LeakyReLU(0.1)',\r\n",
        "    num_inputs = 2,\r\n",
        "    hidden_units = [100, 10, 5],\r\n",
        "    num_outputs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPWedinyGHiY"
      },
      "source": [
        "# ReLU in practice\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OdQ08YiiGk06"
      },
      "source": [
        "#@title Video 6:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyOmkogLKuDt"
      },
      "source": [
        "## Exercise 5: Benchmark Various ReLU Implementation\r\n",
        "Implement and benchmark at least three different ReLU implementations. Use `%timeit` with test number 10 and repeat number 3. Which then takes an average of 10 runs and repeats 3 times, and reports the lowest (best) average time.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jzG8GWvKtLt"
      },
      "source": [
        "x = torch.rand((10000, 10000)).to(dev) - 0.5\r\n",
        "print(\"Pytorch : \", end='')\r\n",
        "%timeit -n10 -r3 torch.relu(x)\r\n",
        "print('-----------------------------------------------')\r\n",
        "\r\n",
        "print(\"First: \", end='')\r\n",
        "%timeit -n10 -r3 torch.max(x, torch.zeros_like(x))   # [TO-DO]\r\n",
        "print('-----------------------------------------------')\r\n",
        "\r\n",
        "print(\"Second: \", end='')\r\n",
        "%timeit -n10 -r3 x * (x > 0)   # [TO-DO]\r\n",
        "print('-----------------------------------------------')\r\n",
        "\r\n",
        "print(\"Third: \", end='')\r\n",
        "%timeit -n10 -r3 x[x<0] = 0   # [TO-DO]\r\n",
        "print('-----------------------------------------------')\r\n",
        "\r\n",
        "print(\"Forth: \", end='')\r\n",
        "%timeit -n10 -r3 (torch.abs(x) + x) / 2   # [TO-DO]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDVqxxaJlz8"
      },
      "source": [
        "# Classification with MLPs\r\n",
        "Two potential loss functions\r\n",
        "## CrossEntropyLoss\r\n",
        "This criterion expects a class index in the range $[0, C-1]$ as the target for each value of a $1D$ tensor of size minibatch. There are other optional parameters like class weights and class ignores. Check the documentation here for more detail. Then in the simplest case, it calculates this,\r\n",
        "\r\n",
        "$$\r\n",
        "\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)\r\n",
        "$$\r\n",
        "\r\n",
        "## MultiMarginLoss\r\n",
        "The loss corresponding to class j is calculated as follows,\r\n",
        "$$\r\n",
        "l_j(x, y)=\\sum_{j\\neq y} \\max (0, \\operatorname{margin}-x[y]+x[j])\r\n",
        "$$\r\n",
        "Then it is averaged over all the class elements and all the mini-batch samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gXlMu7ouaKrb"
      },
      "source": [
        "#@title Video 7:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCvA5etP2GOy"
      },
      "source": [
        "## Exercise 6: Simulate a Spiral Classification Dataset\r\n",
        "Let's turn this fancy-looking equation into a classification dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdmdB6oD0KU"
      },
      "source": [
        "$$\r\n",
        "\\begin{array}{c}\r\n",
        "X_{k}(t)=t\\left(\\begin{array}{c}\r\n",
        "\\sin \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma^{2}\\right) \\\\\r\n",
        "\\cos \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma^{2}\\right) \r\n",
        "\\end{array}\\right)\r\n",
        "\\end{array}, \\quad 0 \\leq t \\leq 1, \\quad k=1, \\ldots, K\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utfWXn81FHT4"
      },
      "source": [
        "K = 4\r\n",
        "sigma = 0.4\r\n",
        "N = 1000\r\n",
        "t = torch.linspace(0, 1, N)\r\n",
        "X = torch.zeros(K*N, 2)\r\n",
        "y = torch.zeros(K*N)\r\n",
        "for k in range(K):\r\n",
        "  X[k*N:(k+1)*N, 0] = t*(torch.sin(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))   # [TO-DO]\r\n",
        "  X[k*N:(k+1)*N, 1] = t*(torch.cos(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))   # [TO-DO]\r\n",
        "  y[k*N:(k+1)*N] = k   # [TO-DO]\r\n",
        "\r\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\r\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUhAlD-XcbTa"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eY2VN0MfcSPN"
      },
      "source": [
        "#@title Video 8:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3RZa-charD2"
      },
      "source": [
        "## Exercise 7: Implement it for Spiral Dataset\r\n",
        "Steps to follow: \r\n",
        "  * Dataset shuffle\r\n",
        "  * Train/Test split\r\n",
        "  * Dataloader definition\r\n",
        "  * Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCS2dEqPfiA"
      },
      "source": [
        "# Shuffling\r\n",
        "shuffled_indeces = torch.randperm(K*N)   # [TO-DO]\r\n",
        "X = X[shuffled_indeces]\r\n",
        "y = y[shuffled_indeces]\r\n",
        "\r\n",
        "# Test Train splitting\r\n",
        "test_size = int(0.2*N)   # [TO-DO]\r\n",
        "X_test = X[:test_size]\r\n",
        "y_test = y[:test_size]\r\n",
        "X_train = X[test_size:]\r\n",
        "y_train = y[test_size:]\r\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\r\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMYnhid1bCyq"
      },
      "source": [
        "And making a Pytorch data loader out of it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJNQtOOgC3E"
      },
      "source": [
        "batch_size = 128\r\n",
        "test_data = TensorDataset(X_test, y_test)\r\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size,\r\n",
        "                         shuffle=False, num_workers=0)\r\n",
        "\r\n",
        "train_data = TensorDataset(X_train, y_train)\r\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True,\r\n",
        "                        shuffle=True, num_workers=0, worker_init_fn=seed_worker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW95_EQp1QHx"
      },
      "source": [
        "def train_test_classification(net, criterion, optimizer,\r\n",
        "                              train_loader, test_loader,\r\n",
        "                              num_epochs=1, verbose=True, \r\n",
        "                              training_plot=False):\r\n",
        "  if verbose:\r\n",
        "    progress_bar = display(progress(0, 0, num_epochs), display_id=True)\r\n",
        "\r\n",
        "  net.train()   # [TO-DO]\r\n",
        "  training_losses = []\r\n",
        "  for epoch in range(num_epochs):  # loop over the dataset multiple times\r\n",
        "      running_loss = 0.0\r\n",
        "      for i, data in enumerate(train_loader, 0):\r\n",
        "          # get the inputs; data is a list of [inputs, labels]\r\n",
        "          inputs, labels = data\r\n",
        "          inputs = inputs.to(dev).float()    # [TO-DO]\r\n",
        "          labels = labels.to(dev).long()   # [TO-DO]\r\n",
        "\r\n",
        "          # zero the parameter gradients\r\n",
        "          optimizer.zero_grad()   # [TO-DO]\r\n",
        "\r\n",
        "          # forward + backward + optimize\r\n",
        "          outputs = net(inputs)   # [TO-DO]\r\n",
        "\r\n",
        "          loss = criterion(outputs, labels)   # [TO-DO]\r\n",
        "          loss.backward()   # [TO-DO]\r\n",
        "          optimizer.step()   # [TO-DO]\r\n",
        "\r\n",
        "          # print statistics\r\n",
        "          if verbose:\r\n",
        "            training_losses += [loss.item()]\r\n",
        "            running_loss += loss.item()\r\n",
        "            if i % 10 == 9:    # update every 10 mini-batches\r\n",
        "                progress_bar.update(progress(epoch+1, running_loss / 10, num_epochs))\r\n",
        "                running_loss = 0.0\r\n",
        "\r\n",
        "  net.eval()   # [TO-DO]\r\n",
        "  def test(data_loader):\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    for data in data_loader:\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(dev).float()\r\n",
        "        labels = labels.to(dev).long()\r\n",
        "\r\n",
        "        outputs = net(inputs)\r\n",
        "        _, predicted = torch.max(outputs, 1)   # [TO-DO]\r\n",
        "        total += labels.size(0)   # [TO-DO]\r\n",
        "        correct += (predicted == labels).sum().item()   # [TO-DO]\r\n",
        "\r\n",
        "    acc = 100 * correct / total    # [TO-DO]\r\n",
        "    return total, acc\r\n",
        "\r\n",
        "  total, train_acc = test(train_loader)\r\n",
        "  total, test_acc = test(test_loader)\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "    print('Accuracy on the %d training samples: %0.2f %%' % (total, train_acc))\r\n",
        "    print('Accuracy on the %d testing samples: %0.2f %%' % (total, test_acc))\r\n",
        "\r\n",
        "  if training_plot:\r\n",
        "    plt.plot(training_losses)\r\n",
        "    plt.xlabel('Batch')\r\n",
        "    plt.ylabel('Training loss')\r\n",
        "    plt.show()\r\n",
        "  \r\n",
        "  return train_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGwbZrUfpZu"
      },
      "source": [
        "net = Net('ReLU()', X_train.shape[1], [128], K).to(dev)    # [TO-DO] \r\n",
        "criterion = nn.CrossEntropyLoss()    # [TO-DO]\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)\r\n",
        "num_epochs = 100\r\n",
        "_, _ = train_test_classification(net, criterion, optimizer, train_loader,\r\n",
        "                                 test_loader, num_epochs=num_epochs,\r\n",
        "                                 training_plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S2ZU9tyeaML"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9Y2J8pw3ejPZ"
      },
      "source": [
        "#@title Video 9:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_HvMx0thX42"
      },
      "source": [
        "## Exercise 8: Implement decision map visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elWAOswugLak"
      },
      "source": [
        "def sample_grid(M=500, x_max = 2.0):\r\n",
        "  ii, jj = torch.meshgrid(torch.linspace(-x_max, x_max,M),\r\n",
        "                          torch.linspace(-x_max, x_max, M))\r\n",
        "  X_all = torch.cat([ii.unsqueeze(-1),\r\n",
        "                     jj.unsqueeze(-1)],\r\n",
        "                     dim=-1).view(-1, 2)    # [TO-DO]\r\n",
        "  return X_all\r\n",
        "\r\n",
        "def plot_decision_map(X_all, y_pred, X_test, y_test, M=500, x_max = 2.0, eps = 1e-3):\r\n",
        "  decision_map = torch.argmax(y_pred, dim=1)    # [TO-DO]\r\n",
        "\r\n",
        "  for i in range(len(X_test)):\r\n",
        "    indeces = (X_all[:, 0] - X_test[i, 0])**2 + (X_all[:, 1] - X_test[i, 1])**2 < eps    # [TO-DO]\r\n",
        "    decision_map[indeces] = (K + y_test[i]).long()    # [TO-DO]\r\n",
        "\r\n",
        "  decision_map = decision_map.view(M, M).cpu()\r\n",
        "  plt.imshow(decision_map, extent=[-x_max, x_max, -x_max, x_max], cmap='jet')\r\n",
        "  plt.plot()\r\n",
        "\r\n",
        "X_all = sample_grid()\r\n",
        "y_pred = net(X_all)\r\n",
        "plot_decision_map(X_all, y_pred, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CwszyupvkL3-"
      },
      "source": [
        "#@title Video 10:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRsWs55XfyAS"
      },
      "source": [
        "## Exercise 9: Implement gradient visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VZUEq01vJJH"
      },
      "source": [
        "def plot_polytopes(grad, M=500, x_max=1):\r\n",
        "  grad = grad.detach().cpu()\r\n",
        "  grad_colors = grad[:, 0]     # [TO-DO]\r\n",
        "  grad_colors = (grad_colors / grad_colors.max() * 1e3).int() % 10     # [TO-DO]\r\n",
        "  grad_colors = grad_colors.view(M, M).cpu().numpy()\r\n",
        "  plt.imshow(grad_colors, cmap='rainbow')\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "rand_net = Net('ReLU()', X_train.shape[1], [128], K).to(dev)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "X_all = sample_grid(x_max=1)\r\n",
        "labels = torch.zeros(len(X_all)).long()\r\n",
        "X_all.requires_grad = True    # [TO-DO]\r\n",
        "outputs = rand_net(X_all)\r\n",
        "\r\n",
        "loss = torch.mean(outputs)    # [TO-DO]\r\n",
        "# loss = torch.mean(outputs**2) # try this to see how it become non-linear\r\n",
        "loss.backward()\r\n",
        "\r\n",
        "plot_polytopes(X_all.grad)    # [TO-DO]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZS17RNAkbmt"
      },
      "source": [
        "## Implement gradient visualization that flows! (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8R71qCfkWhM"
      },
      "source": [
        "def plot_grad(X_all, grad, y_test, M=500):\r\n",
        "  grad = grad.detach().cpu()\r\n",
        "  X_all = X_all.detach().cpu()\r\n",
        "\r\n",
        "  plt.quiver(X_all[:, 0], X_all[:, 1],\r\n",
        "             grad[:, 0], grad[:, 1], y_test)    # [TO-DO]\r\n",
        "  plt.show()\r\n",
        "\r\n",
        "rand_net = Net('ReLU()', X_train.shape[1], [128], K).to(dev)\r\n",
        "X_test.requires_grad = True    # [TO-DO]\r\n",
        "outputs = rand_net(X_test)\r\n",
        "\r\n",
        "loss = criterion(outputs, y_test.long())    # [TO-DO]\r\n",
        "loss.backward()\r\n",
        "\r\n",
        "plot_grad(X_test, X_test.grad, y_test)    # [TO-DO]\r\n",
        "X_test.requires_grad = False    # [TO-DO]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkuxYZ_-lJIr"
      },
      "source": [
        "# MLPs vs Linear model with Polynomial features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bn7efLdVmz8Z"
      },
      "source": [
        "#@title Video 11:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VJgKVfGlaW_"
      },
      "source": [
        "## Exercise 10: Add polynomial features and train without any hidden layers\r\n",
        "[Outline the goal and steps and the formulas, especially how the num_features is calculated]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvqug5B6lmf5"
      },
      "source": [
        "poly_degree = 50\r\n",
        "def make_poly_features(poly_degree, X):\r\n",
        "  num_features = (poly_degree+1)*(poly_degree+2)//2-1   # [TO-DO]\r\n",
        "  poly_X = torch.zeros((X.shape[0], num_features))\r\n",
        "  count = 0\r\n",
        "  for i in range(poly_degree+1):\r\n",
        "    for j in range(poly_degree+1):\r\n",
        "      if j+i > 0: # no need to add zero degree since model has biases\r\n",
        "        if j+i <= poly_degree:\r\n",
        "          poly_X[:, count] = X[:, 0]**i * X[:, 1]**j   # [TO-DO]\r\n",
        "          count += 1\r\n",
        "  return poly_X\r\n",
        "\r\n",
        "poly_X_test = make_poly_features(poly_degree, X_test)\r\n",
        "poly_X_train = make_poly_features(poly_degree, X_train)\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "poly_test_data = TensorDataset(poly_X_test, y_test)\r\n",
        "poly_test_loader = DataLoader(poly_test_data, batch_size=batch_size,\r\n",
        "                        shuffle=False, num_workers=1)\r\n",
        "poly_train_data = TensorDataset(poly_X_train, y_train)\r\n",
        "poly_train_loader = DataLoader(poly_train_data, batch_size=batch_size,\r\n",
        "                        shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Kkn3qtlqaF"
      },
      "source": [
        "poly_net = Net('ReLU()', poly_X_train.shape[1], [], K).to(dev)    # [TO-DO] \r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(poly_net.parameters(), lr=1e-3)\r\n",
        "num_epochs = 100\r\n",
        "_, _ = train_test_classification(poly_net, criterion, optimizer, \r\n",
        "                                 poly_train_loader, poly_test_loader,\r\n",
        "                                 num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUU6oy6emAd3"
      },
      "source": [
        "X_all = sample_grid()\r\n",
        "poly_X_all = make_poly_features(poly_degree, X_all)\r\n",
        "y_pred = poly_net(poly_X_all)\r\n",
        "plot_decision_map(X_all, y_pred, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJYcwHzugx6N"
      },
      "source": [
        "# Wider vs deeper networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5cXchBdhg_r7"
      },
      "source": [
        "#@title Video 12:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG1bGJhXaIpQ"
      },
      "source": [
        "## Exercise 11: Wider vs. Deeper while keeping number of parameters same\r\n",
        "Let's find the optimal number of hidden layers under the constrained fixed number of parameters!\r\n",
        "\r\n",
        "But first, let's implement a model parameter counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4p1WQ9MREW6"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "  # facny implementation\r\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "def count_parameters(model):\r\n",
        "  # more didactic implementation\r\n",
        "  par_count = 0\r\n",
        "  for p in model.parameters():\r\n",
        "    if p.requires_grad:\r\n",
        "      par_count += p.numel()     # [TO-DO]\r\n",
        "  return par_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QhvHQHWYgdY"
      },
      "source": [
        "max_par_count = 100\r\n",
        "max_hidden_layer = 5\r\n",
        "hidden_layers = range(1, max_hidden_layer+1)    # [TO-DO]\r\n",
        "test_scores = []\r\n",
        "for hidden_layer in hidden_layers:\r\n",
        "  hidden_units = np.ones(hidden_layer, dtype=np.int)    # [TO-DO]\r\n",
        "  wide_net = Net('ReLU()', X_train.shape[1], hidden_units, K).to(dev)\r\n",
        "  par_count = count_parameters(wide_net)\r\n",
        "  while par_count < max_par_count:\r\n",
        "    hidden_units += 1\r\n",
        "    wide_net = Net('ReLU()', X_train.shape[1], hidden_units, K).to(dev)\r\n",
        "    par_count = count_parameters(wide_net)\r\n",
        "\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "  optimizer = optim.Adam(wide_net.parameters(), lr=1e-3)\r\n",
        "  num_epochs = 100\r\n",
        "  _, test_acc = train_test_classification(wide_net, criterion, optimizer, train_loader,\r\n",
        "                                          test_loader, num_epochs=num_epochs)    # [TO-DO]\r\n",
        "  test_scores += [test_acc]\r\n",
        "\r\n",
        "plt.xlabel('# of hidden layers')\r\n",
        "plt.ylabel('Test accuracy')\r\n",
        "plt.plot(hidden_layers, test_scores)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfY1h-lFndjt"
      },
      "source": [
        "# Neural Tangent Kernels (NTKs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MNj8KuO7nqIP"
      },
      "source": [
        "#@title Video 13:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPKms1XJWboJ"
      },
      "source": [
        "## Exercise 12: Motivation for NTKs\r\n",
        "lazy training of overcomplete MLPs results in linear changes in weights. Let's try to see it here,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW1wgB3yWaSM"
      },
      "source": [
        "net = Net('ReLU()', X_train.shape[1], [1000], K).to(dev)    # [TO-DO] \r\n",
        "criterion = nn.CrossEntropyLoss()    # [TO-DO]\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-2)    # [TO-DO]\r\n",
        "num_select_weights = 10\r\n",
        "num_time_steps = 5\r\n",
        "step_epoch = 40\r\n",
        "weights = torch.zeros(num_time_steps, num_select_weights)\r\n",
        "for i in range(num_time_steps):\r\n",
        "  _, _ = train_test_classification(net, criterion, optimizer, train_loader,\r\n",
        "                                  test_loader, num_epochs=step_epoch, verbose=False)\r\n",
        "  weights[i] = net.layers[0].weight[:num_select_weights, 0]    # [TO-DO]\r\n",
        "\r\n",
        "for k in range(num_select_weights):\r\n",
        "  weight = weights[:, k].detach()    # [TO-DO]\r\n",
        "  epochs = range(1, 1+num_time_steps*step_epoch, step_epoch)\r\n",
        "  plt.plot(epochs, weight, label='weight #%d'%k)\r\n",
        "\r\n",
        "plt.xlabel('epochs')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "_, _ = train_test_classification(net, criterion, optimizer, train_loader, test_loader, num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8lbbH6upXg6"
      },
      "source": [
        "# Deeper MLPs\r\n",
        "[The ability of deeper MLP to approximate a broader set of functions]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "JIwa78eMpxwL"
      },
      "source": [
        "#@title Video 14:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk_MnhzRDw2t"
      },
      "source": [
        "## Exercise 13: Classification on a real world dataset\r\n",
        "[Outline the goal and steps, introducing the need for augmentation, preprocessing (bring to -1,1 range), small batch_size (due to overfitting), multithreading in data loaders]\r\n",
        "[Ask to choose a good choice for augmentation and preprocessing]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08PjFQyl5yy8"
      },
      "source": [
        "# Data Loaders\r\n",
        "batch_size = 128\r\n",
        "train_transform = transforms.Compose([\r\n",
        "     transforms.RandomRotation(10), # [TO-DO]\r\n",
        "     transforms.RandomHorizontalFlip(),    # [TO-DO]\r\n",
        "     transforms.ToTensor(),\r\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    # [TO-DO] example of a simple one\r\n",
        "     ])\r\n",
        "\r\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\r\n",
        "img_train_dataset = ImageFolder(data_path/'train', transform=train_transform)\r\n",
        "# num_workers can be set to 10 if running on Colab Pro TPUs\r\n",
        "img_train_loader = DataLoader(img_train_dataset, batch_size=batch_size,\r\n",
        "                              shuffle=True, num_workers=10, worker_init_fn=seed_worker)\r\n",
        "\r\n",
        "test_transform = transforms.Compose([\r\n",
        "     transforms.ToTensor(),\r\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    # [TO-DO]\r\n",
        "     ])\r\n",
        "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)\r\n",
        "img_test_loader = DataLoader(img_test_dataset, batch_size=batch_size,\r\n",
        "                        shuffle=False, num_workers=1)\r\n",
        "classes = ('cat', 'dog', 'wild')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JCJbOfsExdJ"
      },
      "source": [
        "# get some random training images\r\n",
        "dataiter = iter(img_train_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "\r\n",
        "# show images\r\n",
        "imshow(make_grid(images, nrow=16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "keBtoq559ceG"
      },
      "source": [
        "net = Net('ReLU()', 3*32*32, [128, 32], 3).to(dev) \r\n",
        "criterion = nn.MultiMarginLoss(margin=1.0)    # [TO-DO]\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\r\n",
        "_, _ = train_test_classification(net, criterion, optimizer,\r\n",
        "                                img_train_loader, img_test_loader,\r\n",
        "                                num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeM-_Xr8-d9F"
      },
      "source": [
        "fc1_weights = net.layers[0].weight.view(128, 3, 32, 32).detach().cpu()     # [TO-DO]\r\n",
        "fc1_weights /= torch.max(torch.abs(fc1_weights))\r\n",
        "imshow(make_grid(fc1_weights, nrow=16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aQSP-_Yy4Hm"
      },
      "source": [
        "# The choice of transfer function matters\r\n",
        "[introduce different properties of different transfer functions]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "zFG-Oz1P3FoS"
      },
      "source": [
        "#@title Video 15:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc6M5Ubf4mIo"
      },
      "source": [
        "## Exercise 14: Find the best transfer function for this model\r\n",
        "[categorizing Pytorch transfer functions according to their properties in a table so that students could have a better educated guess on which one to pick instead of trying all!]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmgQtA534dn"
      },
      "source": [
        "# Possible Activations\r\n",
        "Activations = ['ReLU', 'Tanh', 'Sigmoid', 'ELU', 'Hardshrink', 'Hardsigmoid',\r\n",
        "'Hardtanh', 'Hardswish', 'LeakyReLU', 'LogSigmoid', 'PReLU',\r\n",
        "'ReLU6', 'RReLU', 'SELU', 'CELU', 'GELU', 'SiLU', 'Softplus',\r\n",
        "'Softshrink', 'Softsign', 'Tanhshrink']\r\n",
        "\r\n",
        "your_picks = ['Hardswish'] # [TO-DO] other picks above 91.5% test accuracy is acceptable\r\n",
        "\r\n",
        "for actv in your_picks:\r\n",
        "  print(actv)\r\n",
        "  actv = actv+'()'\r\n",
        "  net = Net(actv, 3*32*32, [128, 32], 3).to(dev) \r\n",
        "  criterion = nn.MultiMarginLoss(margin=1.0)\r\n",
        "  optimizer = optim.Adam(net.parameters(), lr=3e-4)\r\n",
        "  _, _ = train_test_classification(net, criterion, optimizer,\r\n",
        "                                  img_train_loader, img_test_loader,\r\n",
        "                                  num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7kyUEBxHXnd"
      },
      "source": [
        "fc1_weights = net.layers[0].weight.view(128, 3, 32, 32).detach().cpu()\r\n",
        "fc1_weights /= torch.max(torch.abs(fc1_weights))\r\n",
        "imshow(make_grid(fc1_weights, nrow=16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaE5OQ_39jRH"
      },
      "source": [
        "# The need for good initialization\r\n",
        "[The discussion about why optimal gain is dependent with the transfer functions and what is theoretical optimal solution for Leaky ReLU]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fsZzbd099ltE"
      },
      "source": [
        "#@title Video 16:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40KCBDx686kr"
      },
      "source": [
        "## Xavier initialization\r\n",
        "Let us look at the scale distribution of an output (e.g., a hidden variable)  $o_i$  for some fully-connected layer without nonlinearities. With  $n_{in}$  inputs  ($x_j$)  and their associated weights  $w_{ij}$  for this layer. Then an output is given by,\r\n",
        "$$\r\n",
        "o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j\r\n",
        "$$\r\n",
        "The weights  $w_{ij}$  are all drawn independently from the same distribution. Furthermore, let us assume that this distribution has zero mean and variance  $\\sigma^2$ . Note that this does not mean that the distribution has to be Gaussian, just that the mean and variance need to exist. For now, let us assume that the inputs to the layer  $x_j$ also have zero mean and variance  $\\gamma^2$  and that they are independent of $w_{ij}$ and independent of each other. In this case, we can compute the mean and variance of $o_i$ as follows:\r\n",
        "\\begin{split}\\begin{aligned}\r\n",
        "    E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] = 0, \\\\\r\n",
        "    \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] = n_\\mathrm{in} \\sigma^2 \\gamma^2\r\n",
        "\\end{aligned}\\end{split}\r\n",
        "One way to keep the variance fixed is to set $n_{in}\\sigma^2=1$ . Now consider backpropagation. There we face a similar problem, albeit with gradients being propagated from the layers closer to the output. Using the same reasoning as for forward propagation, we see that the gradientsâ€™ variance can blow up unless $n_{out}\\sigma^2=1$ , where  $n_{out}$ is the number of outputs of this layer. This leaves us in a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead, we simply try to satisfy:\r\n",
        "\\begin{aligned}\r\n",
        "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ or equivalently }\r\n",
        "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}\r\n",
        "\\end{aligned}\r\n",
        "This is the reasoning underlying the now-standard and practically beneficial Xavier initialization, named after the first author of its creators [Glorot & Bengio, 2010]. Typically, the Xavier initialization samples weights from a Gaussian distribution with zero mean and variance  $\\sigma^2=\\frac{2}{(n_{in}+n_{out})}$. We can also adapt Xavierâ€™s intuition to choose the variance when sampling weights from a uniform distribution. Note that the uniform distribution $U(âˆ’a,a)$ has variance $\\frac{a^2}{3}$. Plugging this into our condition on $\\sigma^2$ yields the suggestion to initialize according to\r\n",
        "$$\r\n",
        "U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right)\r\n",
        "$$\r\n",
        "This explanation is mainly taken from [here](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm5fWWd6j7Y1"
      },
      "source": [
        "## Initialization with transfer function\r\n",
        "Let's derive the optimal gain for LeakyReLU following a similar steps,\r\n",
        "\r\n",
        "$$\r\n",
        "f(x)=\\left\\{\\begin{array}{ll}\r\n",
        "a x & \\text { for } x<0 \\\\\r\n",
        "x & \\text { for } x \\geq 0\r\n",
        "\\end{array}\\right.\r\n",
        "$$\r\n",
        "\r\n",
        "Considering a single layer with activation gives, \r\n",
        "\r\n",
        "\r\n",
        "The expectation of the output is still zero but the variance changes and assuming the probability $P(x < 0) = 0.5$\r\n",
        "\r\n",
        "\\begin{split}\\begin{aligned}\r\n",
        "    \\mathrm{Var}[f(o_i)] = E[f(o_i)^2] & = \\frac{\\mathrm{Var}[o_i] + a^2 \\mathrm{Var}[o_i]}{2} = \\frac{1+a^2}{2}n_\\mathrm{in} \\sigma^2 \\gamma^2\r\n",
        "\\end{aligned}\\end{split}\r\n",
        "\r\n",
        "Therefore following the rest of derivation as before,\r\n",
        "\r\n",
        "$$\r\n",
        "\\sigma = gain\\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\quad gain = \\sqrt{\\frac{2}{1+a^2}}\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LxZCVS5pCsD"
      },
      "source": [
        "## Exercise 15: Best gain for Xavier Initialization with Leaky ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqeL_3FuHn3X"
      },
      "source": [
        "N = 10 # number of trials\r\n",
        "gains = np.linspace(1/N, 3.0, N)\r\n",
        "test_accs = []\r\n",
        "train_accs = []\r\n",
        "for gain in gains:\r\n",
        "\r\n",
        "  def init_weights(m):\r\n",
        "    if type(m) == nn.Linear:\r\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain) # [TO-DO]\r\n",
        "        # torch.nn.init.xavier_uniform_(m.weight, gain)\r\n",
        "\r\n",
        "  negative_slope = 0.1\r\n",
        "  actv = 'LeakyReLU(%f)'%negative_slope # [TO-DO]\r\n",
        "  net = Net(actv, 3*32*32, [128, 64, 32], 3).to(dev) \r\n",
        "  net.apply(init_weights) # [TO-DO]\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "  # criterion = nn.MultiMarginLoss(margin=1.0)\r\n",
        "  optimizer = optim.SGD(net.parameters(), lr=1e-2) # [TO-DO]\r\n",
        "  train_acc, test_acc = train_test_classification(net, criterion, optimizer,\r\n",
        "                                                  img_train_loader, img_test_loader,\r\n",
        "                                                  num_epochs=1, verbose=False)\r\n",
        "  test_accs += [test_acc]\r\n",
        "  train_accs += [train_acc]\r\n",
        "\r\n",
        "best_gain = gains[np.argmax(train_accs)]\r\n",
        "plt.plot(gains, test_accs, label='Test')\r\n",
        "plt.plot(gains, train_accs, label='Train')\r\n",
        "plt.scatter(best_gain, max(train_accs), label='argmax gain = %.1f'%best_gain, c='r')\r\n",
        "theoretical_gain = np.sqrt(2.0 / (1 + negative_slope ** 2)) # [TO-DO]\r\n",
        "plt.scatter(theoretical_gain, max(train_accs), label='theoretical gain = %.2f'%theoretical_gain, c='g')\r\n",
        "plt.legend()\r\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMxtE7y0AF1o"
      },
      "source": [
        "# Conclusion\r\n",
        "[Overcomplete MLPs are good (make link to neural tangent kernels), show how the infinite width limit produces beautifully smooth interpolations]\r\n",
        "\r\n",
        "[High dimensional spaces intuition, the idea of ensemble methods, See how mixing multiple models often helps. Conceptualize ANNs as many models in parallel]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "9x3YnWphApIa"
      },
      "source": [
        "#@title Video 17:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kglwCDy7x71"
      },
      "source": [
        "#Feedback\r\n",
        "how could this session have been better? How happy are you in your group? How do you feel right now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RWrOEFlb730A"
      },
      "source": [
        "# report to Airtable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGTLB5JFJ4-k"
      },
      "source": [
        "# Homeworks\r\n",
        "* (1) Join the Kaggle Competition to solve Animal Faces with random permutations using MLPs.\r\n",
        "* (2) Something where you debug something: Cross-entropy optimization with poor initialization, producing NaNs.\r\n",
        "* (3) Something related to ethics: A classification system with interest bias?\r\n",
        "* (4) Read some cool original paper:  Kernel vs Rich regimes paper?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPmj2vUfv4t7"
      },
      "source": [
        "# Kaggle competition\r\n",
        "https://www.kaggle.com/c/permuted-animal-faces/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh0C1P5EPpvl"
      },
      "source": [
        "preparing the Kaggle dataset by permuting the animal faces\r\n",
        "\r\n",
        "[ To-be removed in the end]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qz6r1kAZ3qRK"
      },
      "source": [
        "\r\n",
        "# Datasets\r\n",
        "train_transform = transforms.Compose([\r\n",
        "     transforms.RandomRotation(10),\r\n",
        "     transforms.RandomHorizontalFlip(),\r\n",
        "     ])\r\n",
        "\r\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\r\n",
        "img_train_dataset = ImageFolder(data_path/'train', transform=train_transform)\r\n",
        "\r\n",
        "test_transform = transforms.Compose([\r\n",
        "     transforms.RandomRotation(10),\r\n",
        "     transforms.RandomHorizontalFlip(),\r\n",
        "     ])\r\n",
        "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJjmMQ_6riA"
      },
      "source": [
        "rand_perm = np.random.permutation(3*32*32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKR2KfQ_PzHd"
      },
      "source": [
        "import json\r\n",
        "with open(\"rand_perm.txt\", \"w\") as fp:\r\n",
        "  json.dump(rand_perm.tolist(), fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxGty2FtyEtq"
      },
      "source": [
        "test_data = np.zeros((len(img_test_dataset), 1+3*32*32),\r\n",
        "                     dtype=np.uint8)\r\n",
        "for i, data in enumerate(img_test_dataset):\r\n",
        "  X, y = data\r\n",
        "  X = np.array(X, dtype=np.uint8).reshape(-1)\r\n",
        "  X = X[rand_perm]\r\n",
        "  test_data[i, 0] = y\r\n",
        "  test_data[i, 1:] = X\r\n",
        "test_data = np.random.permutation(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4lOgx_T8BZD"
      },
      "source": [
        "header = []\r\n",
        "for i in range(3*32*32):\r\n",
        "  header += ['pixel%d'%(i+1)]\r\n",
        "\r\n",
        "import csv\r\n",
        "with open('test.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(header)\r\n",
        "    \r\n",
        "    for i in range(test_data.shape[0]):\r\n",
        "      data = test_data[i, 1:].tolist()\r\n",
        "      writer.writerow(data)\r\n",
        "\r\n",
        "header = ['ImageId', 'Label']\r\n",
        "with open('solution.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(header)\r\n",
        "    \r\n",
        "    for i in range(test_data.shape[0]):\r\n",
        "      data = [i+1] + [test_data[i, 0]]\r\n",
        "      writer.writerow(data)\r\n",
        "\r\n",
        "header = ['ImageId', 'Label']\r\n",
        "with open('sampleSubmission.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(header)\r\n",
        "    \r\n",
        "    for i in range(test_data.shape[0]):\r\n",
        "      writer.writerow([i+1] + [1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL5aNMmWwntM"
      },
      "source": [
        "train_data = np.zeros((len(img_train_dataset), 1+3*32*32),\r\n",
        "                     dtype=np.uint8)\r\n",
        "for i, data in enumerate(img_train_dataset):\r\n",
        "  X, y = data\r\n",
        "  X = np.array(X, dtype=np.uint8).reshape(-1)\r\n",
        "  X = X[rand_perm]\r\n",
        "  train_data[i, 0] = y\r\n",
        "  train_data[i, 1:] = X\r\n",
        "train_data = np.random.permutation(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtDNXulZAlCg"
      },
      "source": [
        "header = [\"Label\"]\r\n",
        "for i in range(3*32*32):\r\n",
        "  header += ['pixel %d'%(i+1)]\r\n",
        "\r\n",
        "with open('train.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow(header)\r\n",
        "    \r\n",
        "    for i in range(train_data.shape[0]):\r\n",
        "      data = train_data[i].tolist()\r\n",
        "      writer.writerow(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}